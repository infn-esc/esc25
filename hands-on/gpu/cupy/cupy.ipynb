{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3510b04c",
   "metadata": {},
   "source": [
    "## Exercise 1\n",
    "\n",
    "Move this code from `numpy` to `cupy`. Note that GPU operations are asynchronous!\n",
    "\n",
    "Rembember to `cp.cuda.Device().synchronize()` when timing the execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b20d064",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "\n",
    "N = 100_000_000\n",
    "\n",
    "# Create arrays on CPU\n",
    "a = np.ones(N, dtype=np.float32)\n",
    "b = np.ones(N, dtype=np.float32) * 2\n",
    "\n",
    "# Perform vector addition\n",
    "start = time.time()\n",
    "c = a + b + np.sin(a)\n",
    "end = time.time()\n",
    "\n",
    "print(\"Sum of all elements:\", np.sum(c))\n",
    "print(\"Time (CPU):\", end - start, \"seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94588e59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# do the same with cupy now"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adc61c10",
   "metadata": {},
   "source": [
    "## Exercise 2\n",
    "\n",
    "Let's play with copies now!\n",
    "\n",
    "Fill the gaps for the creation and copies of the arrays. Then you can create the arrays directly on the GPU to avoid the host to device copy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f83c00b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cupy as cp\n",
    "import time\n",
    "\n",
    "N = 100_000_000 \n",
    "\n",
    "# create two arrays of N random numbers on CPU\n",
    "h_a = # TODO\n",
    "h_b = # TODO\n",
    "\n",
    "# copy it to GPU\n",
    "start_htod = time.time()\n",
    "d_a = # TODO\n",
    "d_b = # TODO\n",
    "\n",
    "# synchronize\n",
    "\n",
    "end_htod = time.time()\n",
    "\n",
    "print(f\"Host to Device copy: {(end_htod - start_htod)*1e3:.2f} ms\")\n",
    "\n",
    "# perform some operation on the GPU\n",
    "d_c = d_a + d_b\n",
    "\n",
    "# copy back to the CPU\n",
    "start_dtoh = time.time()\n",
    "h_c_from_gpu = # TODO \n",
    "\n",
    "# synchronize\n",
    "\n",
    "end_dtoh = time.time()\n",
    "print(f\"Device to Host copy: {(end_dtoh - start_dtoh)*1e3:.2f} ms\")\n",
    "\n",
    "# compare results\n",
    "h_c = h_a + h_b\n",
    "if np.allclose(h_c, h_c_from_gpu, rtol=1e-5):\n",
    "    print(\"\\nPassed :)\")\n",
    "else:\n",
    "    print(\"\\nFailed :(\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fba371f",
   "metadata": {},
   "source": [
    "## Exercise 3\n",
    "\n",
    "Take your favorite kernel and write it with cupy using `cp.RawKernel` or `cp.ElementwiseKernel` (or try filling an array, summing two arrays..).\n",
    "\n",
    "Remember that with `cp.ElementwiseKernel` you can use the special variable `i` for the the index within the loop and method `_ind.size()` for the total number of elements to apply the elementwise operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efe0aa5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cupy as cp\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d506c5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Raw CUDA kernel\n",
    "kernel_code = r\"\"\"\n",
    "// kernel code\n",
    "\"\"\"\n",
    "\n",
    "mykernel = cp.RawKernel(kernel_code, \"mykernel\")\n",
    "\n",
    "# Launch configuration\n",
    "threads_per_block = 256\n",
    "blocks_per_grid = # TODO\n",
    "\n",
    "# Launch kernel\n",
    "fill_indices_kernel((blocks_per_grid,), \n",
    "                    (threads_per_block,), \n",
    "                    ('''kernel arguments'''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93f6e16e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CuPy's elementwise kernel\n",
    "fill_indices_kernel = cp.ElementwiseKernel(\n",
    "    '',  # list of inputs \n",
    "    '',  # list of outputs\n",
    "    '',  # operation\n",
    "    ''   # kernel name\n",
    ")\n",
    "\n",
    "# Launch the elementwise kernel\n",
    "fill_indices_kernel('''input and outputs''')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2779beee",
   "metadata": {},
   "source": [
    "## Exercise 4\n",
    "\n",
    "Reduction time!\n",
    "\n",
    "NOTE: you can use `cp.dtype(cp.float32).itemsize` to obtain the size of a `float` in bytes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c918f2b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cupy as cp\n",
    "import numpy as np\n",
    "\n",
    "N = 1024 \n",
    "a = cp.arange(N, dtype=cp.float32)\n",
    "\n",
    "block_size = 256\n",
    "grid_size = # TODO\n",
    "partial_sums = # TODO (an empty array of ? elements to store the partial sums on GPU)\n",
    "\n",
    "# CUDA kernel: reduce a block of input into shared memory\n",
    "kernel_code = r'''\n",
    "// your kernel code\n",
    "'''\n",
    "\n",
    "reduce_kernel = cp.RawKernel(kernel_code, \"reduce_sum\")\n",
    "\n",
    "# First reduction pass over the input array\n",
    "shared_mem_size = # TODO (how much shared memory is needed?)\n",
    "reduce_kernel((grid_size,), (block_size,),\n",
    "              (a, partial_sums, np.int32(N)),\n",
    "              shared_mem=shared_mem_size)\n",
    "\n",
    "# Second reduction pass over partial_sums to get final result\n",
    "# We can use the same kernel to reduce the partial sums array\n",
    "partial_sums_out = # TODO (an empty array of ? elements to store the result on GPU)\n",
    "\n",
    "grid_size = # TODO\n",
    "reduce_kernel((grid_size,), (block_size,),\n",
    "              (partial_sums, partial_sums_out, np.int32(grid_size)),\n",
    "              shared_mem=shared_mem_size)\n",
    "\n",
    "cp.cuda.Device().synchronize()  # ensure GPU work is complete\n",
    "\n",
    "# Verify\n",
    "expected = np.sum(cp.asnumpy(a))\n",
    "computed = float(partial_sums_out[0])\n",
    "\n",
    "if np.allclose(computed, expected):\n",
    "    print(\"\\nPassed!\")\n",
    "else:\n",
    "    print(\"RawKernel reduction:\", computed)\n",
    "    print(\"Expected:           \", expected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e27c202e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cupy as cp\n",
    "import numpy as np\n",
    "\n",
    "N = 1024\n",
    "a = cp.arange(N, dtype=cp.float32)\n",
    "\n",
    "# with reduction kernel\n",
    "reduction_kernel = cp.ReductionKernel(\n",
    "    '',      # input params\n",
    "    '',      # output param\n",
    "    '',      # map\n",
    "    '',      # reduce\n",
    "    '',      # post-reduction\n",
    "    '',      # identity value\n",
    "    ''       # kernel name\n",
    ")\n",
    "total = reduction_kernel(a)\n",
    "\n",
    "# with built in python function\n",
    "total2 = # TODO\n",
    "\n",
    "expected = np.sum(cp.asnumpy(a))\n",
    "if np.allclose(total, expected):\n",
    "    print(\"\\nReductionKernel passed!\")\n",
    "if np.allclose(total2, expected):\n",
    "    print(\"\\ncp.sum passed!\")"
   ]
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
